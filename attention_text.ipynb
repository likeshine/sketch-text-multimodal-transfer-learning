{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1637076985885
    },
    "id": "3K7u2BRsyVPT"
   },
   "outputs": [],
   "source": [
    "##https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb#scrollTo=ZTO-30-IMfK3\n",
    "##how to feed grayscale images to pretrained models which requires 3 input channels:\n",
    "##https://stackoverflow.com/questions/51995977/how-can-i-use-a-pre-trained-neural-network-with-grayscale-images\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import platform\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "# mogrify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eGwGQ7dyVPW",
    "outputId": "b8c519f9-7aee-40d2-bc12-e2f624380f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0:2], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y4OfBhDgYF-_"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wVND9sRYyh-B"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1637077095460
    },
    "id": "pxpkHzj3yVPX",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sketch = pd.read_csv('data/sketch_drawing.csv')\n",
    "\n",
    "var = 'Unique_expert'\n",
    "# var = 'Drawing_expert'\n",
    "# var = 'Useful_expert'\n",
    "# var = 'Elegence_expert'\n",
    "# var = 'Creativity_expert'\n",
    "\n",
    "mapping1 = {'Creativity_expert': 'Creativity',\n",
    "                       'Useful_expert': 'Useful',\n",
    "                       'Unique_expert': 'Unique',\n",
    "                       'Drawing_expert': 'Drawing',\n",
    "                       'Elegence_expert': 'Elegence',}\n",
    "mapping = {'Creativity_expert': 'Creativity_expert_round',\n",
    "                       'Useful_expert': 'Useful_expert_round',\n",
    "                       'Unique_expert': 'Unique_expert_round',\n",
    "                       'Drawing_expert': 'Drawing_expert_round',\n",
    "                       'Elegence_expert': 'Elegence_expert_round',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1637078107788
    },
    "id": "pkdOkCFOyVPY",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sketch1 = pd.read_csv('data/sketch_drawing.csv')\n",
    "train_df = sketch1[sketch1['set2_'+var] == 'train'] \n",
    "val_df = sketch1[sketch1['set2_'+var] == 'val'] \n",
    "test_df = sketch1[sketch1['set2_'+var] == 'test'] \n",
    "\n",
    "# train_graph = [images[i] for i in train_df.index]\n",
    "# val_graph = [images[i] for i in val_df.index]\n",
    "# test_graph = [images[i] for i in test_df.index]\n",
    "\n",
    "##to add\n",
    "sketch1 = sketch1.astype({\"text\": str})\n",
    "text = list(sketch1['text'])\n",
    "train_text = [text[i] for i in train_df.index]\n",
    "val_text = [text[i] for i in val_df.index]\n",
    "test_text = [text[i] for i in test_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2Lpd8sVlyVPY"
   },
   "outputs": [],
   "source": [
    "train_cost = train_df[var]\n",
    "val_cost = val_df[var]\n",
    "test_cost = test_df[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BHWfxDFtyVPZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "def get_bin_idx(x):\n",
    "    return min(int(x * np.float32(10)), 99)\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g8i8s18AyVPZ"
   },
   "outputs": [],
   "source": [
    "# https://github.com/YyzHarry/imbalanced-regression''\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "for i in [0]:\n",
    "    # preds, labels: [Ns,], \"Ns\" is the number of total samples\n",
    "#     preds, labels = train_df, train_cost1[var]\n",
    "    labels = train_cost\n",
    "    # assign each label to its corresponding bin (start from 0)\n",
    "    # with your defined get_bin_idx(), return bin_index_per_label: [Ns,] \n",
    "    bin_index_per_label = [get_bin_idx(label) for label in labels]\n",
    "\n",
    "    # calculate empirical (original) label distribution: [Nb,]\n",
    "    # \"Nb\" is the number of bins\n",
    "    Nb = max(bin_index_per_label) + 1\n",
    "    num_samples_of_bins = dict(Counter(bin_index_per_label))\n",
    "    emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]\n",
    "\n",
    "    # lds_kernel_window: [ks,], here for example, we use gaussian, ks=5, sigma=2\n",
    "    lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2)\n",
    "    # calculate effective label distribution: [Nb,]\n",
    "    eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "\n",
    "    # Use re-weighting based on effective label distribution, sample-wise weights: [Ns,]\n",
    "    eff_num_per_label = [eff_label_dist[bin_idx] for bin_idx in bin_index_per_label]\n",
    "    weights = [np.float32(1 / x) for x in eff_num_per_label]\n",
    "    # # calculate loss\n",
    "    # loss = weighted_mse_loss(preds, labels, weights=weights)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lz88jgMryVPa",
    "outputId": "df08b488-d9a7-4430-94a9-6982aae00aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rppdIUuI7QTg"
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3xRUTVpUyVPb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "bert_preprocess_model = hub.KerasLayer(\"model weight/bert_en_uncased_preprocess_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "R_wS0cgAyVPb"
   },
   "outputs": [],
   "source": [
    "# text_preprocessed_train = bert_preprocess_model(train_text)\n",
    "# text_preprocessed_val = bert_preprocess_model(val_text)\n",
    "# text_preprocessed_test = bert_preprocess_model(test_text)\n",
    "\n",
    "# text_test = ['this is such an amazing movie!']\n",
    "# text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "# print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "# print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8u3XWGvH1QHd"
   },
   "outputs": [],
   "source": [
    "# pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras import Model, Input, layers, regularizers\n",
    "# from keras.models import load_model\n",
    "# from tensorflow.keras import activations\n",
    "# from official.nlp import optimization \n",
    "\n",
    "# epochs = 1000\n",
    "# steps_per_epoch = 56\n",
    "# num_train_steps = steps_per_epoch * epochs\n",
    "\n",
    "# ##unique: 0, drawing: 0.05, elegance: 0.05, creativity: 0.05, useful: 0.1\n",
    "# a = 0.005\n",
    "# # if var == 'Unique_expert':\n",
    "# #     a = 0\n",
    "# # elif var == 'Creativity_expert':\n",
    "# #     a = 0.02\n",
    "# # elif var == 'Useful_expert':\n",
    "# #     a = 0.1\n",
    "# num_warmup_steps = int(a*num_train_steps)\n",
    "# init_lr = 2e-5\n",
    "# optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "#                                           num_train_steps=num_train_steps,\n",
    "#                                           num_warmup_steps=num_warmup_steps,\n",
    "#                                           optimizer_type='adamw')\n",
    "\n",
    "# text_input = [Input(shape=(), dtype=tf.string, name='text')]\n",
    "# preprocessor = hub.load(\"model weight/bert_en_uncased_preprocess_3\")\n",
    "# tokenize = hub.KerasLayer(preprocessor.tokenize)\n",
    "# tokenized_inputs = [tokenize(segment) for segment in text_input]\n",
    "# seq_length = 32\n",
    "# bert_pack_inputs = hub.KerasLayer(\n",
    "#     preprocessor.bert_pack_inputs,\n",
    "#     arguments=dict(seq_length=seq_length))  # Optional argument.\n",
    "# encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "# encoder = hub.KerasLayer(\"model weight/small_bert_bert_en_uncased_L-4_H-512_A-8_2\", trainable=True, name='BERT_encoder')\n",
    "# outputs = encoder(encoder_inputs)\n",
    "# net = outputs['pooled_output']\n",
    "# # net = layers.Dropout(0.1)(net)\n",
    "# net = layers.Dense(64, activation='relu')(net)\n",
    "# net = layers.Dropout(0.1)(net)\n",
    "# out1y = layers.Dense(1, activation='relu', name='out1y')(net)\n",
    "# model = Model(text_input, out1y)\n",
    "\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss=tf.keras.losses.MeanSquaredError(),\n",
    "#               metrics= tf.keras.metrics.RootMeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "AY_44A5qyVPc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import Model, Input, layers, regularizers\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import activations\n",
    "from official.nlp import optimization \n",
    "\n",
    "epochs = 1000\n",
    "steps_per_epoch = 56\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "\n",
    "##unique: 0, drawing: 0.05, elegance: 0.05, creativity: 0.05, useful: 0.1\n",
    "a = 0.05\n",
    "# if var == 'Unique_expert':\n",
    "#     a = 0\n",
    "# elif var == 'Creativity_expert':\n",
    "#     a = 0.02\n",
    "# elif var == 'Useful_expert':\n",
    "#     a = 0.1\n",
    "num_warmup_steps = int(a*num_train_steps)\n",
    "init_lr = 2e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "text_input = Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessing_layer = hub.KerasLayer(\"model weight/bert_en_uncased_preprocess_3\", name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(\"model weight/small_bert_bert_en_uncased_L-4_H-512_A-8_2\", trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "# net = layers.Dropout(0.1)(net)\n",
    "net = layers.Dense(64, activation='relu')(net)\n",
    "net = layers.Dropout(0.1)(net)\n",
    "out1y = layers.Dense(1, activation='relu', name='out1y')(net)\n",
    "model = Model(text_input, out1y)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics= tf.keras.metrics.RootMeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "zTi2WPRlyVPd"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "1eLZlR3MyVPd"
   },
   "outputs": [],
   "source": [
    "##definition of callbacks\n",
    "epochs = 1000\n",
    "steps_per_epoch = 56\n",
    "batch_size = 24\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\\\n",
    "    patience=20,\n",
    "    monitor=\"val_loss\",\n",
    "    restore_best_weights=True,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIYL1FPgyVPd",
    "outputId": "fb513724-d363-4861-f64e-b553da7d81d5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "56/56 [==============================] - 18s 257ms/step - loss: 0.0763 - root_mean_squared_error: 2.1277 - val_loss: 1.9718 - val_root_mean_squared_error: 1.4042\n",
      "Epoch 2/1000\n",
      "56/56 [==============================] - 14s 243ms/step - loss: 0.0339 - root_mean_squared_error: 1.4557 - val_loss: 1.8431 - val_root_mean_squared_error: 1.3576\n",
      "Epoch 3/1000\n",
      "56/56 [==============================] - 14s 242ms/step - loss: 0.0319 - root_mean_squared_error: 1.3892 - val_loss: 1.8913 - val_root_mean_squared_error: 1.3752\n",
      "Epoch 4/1000\n",
      "56/56 [==============================] - 13s 241ms/step - loss: 0.0254 - root_mean_squared_error: 1.2630 - val_loss: 1.6480 - val_root_mean_squared_error: 1.2837\n",
      "Epoch 5/1000\n",
      "56/56 [==============================] - 14s 256ms/step - loss: 0.0218 - root_mean_squared_error: 1.1977 - val_loss: 1.5332 - val_root_mean_squared_error: 1.2382\n",
      "Epoch 6/1000\n",
      "56/56 [==============================] - 14s 242ms/step - loss: 0.0187 - root_mean_squared_error: 1.1227 - val_loss: 1.8017 - val_root_mean_squared_error: 1.3423\n",
      "Epoch 7/1000\n",
      "56/56 [==============================] - 14s 244ms/step - loss: 0.0128 - root_mean_squared_error: 0.9750 - val_loss: 1.6043 - val_root_mean_squared_error: 1.2666\n",
      "Epoch 8/1000\n",
      "56/56 [==============================] - 14s 248ms/step - loss: 0.0102 - root_mean_squared_error: 0.8801 - val_loss: 1.5676 - val_root_mean_squared_error: 1.2521\n",
      "Epoch 9/1000\n",
      "56/56 [==============================] - 14s 242ms/step - loss: 0.0081 - root_mean_squared_error: 0.7899 - val_loss: 1.5547 - val_root_mean_squared_error: 1.2469\n",
      "Epoch 10/1000\n",
      "56/56 [==============================] - 13s 239ms/step - loss: 0.0083 - root_mean_squared_error: 0.8114 - val_loss: 1.5576 - val_root_mean_squared_error: 1.2480\n",
      "Epoch 11/1000\n",
      "56/56 [==============================] - 13s 236ms/step - loss: 0.0066 - root_mean_squared_error: 0.7242 - val_loss: 1.4756 - val_root_mean_squared_error: 1.2148\n",
      "Epoch 12/1000\n",
      "56/56 [==============================] - 13s 231ms/step - loss: 0.0058 - root_mean_squared_error: 0.6664 - val_loss: 1.4989 - val_root_mean_squared_error: 1.2243\n",
      "Epoch 13/1000\n",
      "56/56 [==============================] - 14s 257ms/step - loss: 0.0055 - root_mean_squared_error: 0.6444 - val_loss: 1.4832 - val_root_mean_squared_error: 1.2179\n",
      "Epoch 14/1000\n",
      "56/56 [==============================] - 14s 246ms/step - loss: 0.0053 - root_mean_squared_error: 0.6481 - val_loss: 1.5296 - val_root_mean_squared_error: 1.2368\n",
      "Epoch 15/1000\n",
      "56/56 [==============================] - 13s 233ms/step - loss: 0.0048 - root_mean_squared_error: 0.6077 - val_loss: 1.4772 - val_root_mean_squared_error: 1.2154\n",
      "Epoch 16/1000\n",
      "56/56 [==============================] - 14s 241ms/step - loss: 0.0044 - root_mean_squared_error: 0.5835 - val_loss: 1.4257 - val_root_mean_squared_error: 1.1940\n",
      "Epoch 17/1000\n",
      "56/56 [==============================] - 14s 253ms/step - loss: 0.0041 - root_mean_squared_error: 0.5649 - val_loss: 1.4274 - val_root_mean_squared_error: 1.1947\n",
      "Epoch 18/1000\n",
      "56/56 [==============================] - 13s 236ms/step - loss: 0.0045 - root_mean_squared_error: 0.5794 - val_loss: 1.4354 - val_root_mean_squared_error: 1.1981\n",
      "Epoch 19/1000\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 0.0043 - root_mean_squared_error: 0.5673 - val_loss: 1.4317 - val_root_mean_squared_error: 1.1965\n",
      "Epoch 20/1000\n",
      "56/56 [==============================] - 12s 208ms/step - loss: 0.0036 - root_mean_squared_error: 0.5338 - val_loss: 1.4336 - val_root_mean_squared_error: 1.1973\n",
      "Epoch 21/1000\n",
      "56/56 [==============================] - 12s 211ms/step - loss: 0.0038 - root_mean_squared_error: 0.5253 - val_loss: 1.4263 - val_root_mean_squared_error: 1.1943\n",
      "Epoch 22/1000\n",
      "56/56 [==============================] - 12s 209ms/step - loss: 0.0036 - root_mean_squared_error: 0.5264 - val_loss: 1.4273 - val_root_mean_squared_error: 1.1947\n",
      "Epoch 23/1000\n",
      "56/56 [==============================] - 12s 217ms/step - loss: 0.0035 - root_mean_squared_error: 0.5159 - val_loss: 1.3759 - val_root_mean_squared_error: 1.1730\n",
      "Epoch 24/1000\n",
      "56/56 [==============================] - 12s 217ms/step - loss: 0.0032 - root_mean_squared_error: 0.4985 - val_loss: 1.4074 - val_root_mean_squared_error: 1.1864\n",
      "Epoch 25/1000\n",
      "56/56 [==============================] - 12s 220ms/step - loss: 0.0036 - root_mean_squared_error: 0.5191 - val_loss: 1.3666 - val_root_mean_squared_error: 1.1690\n",
      "Epoch 26/1000\n",
      "56/56 [==============================] - 12s 209ms/step - loss: 0.0036 - root_mean_squared_error: 0.5112 - val_loss: 1.3973 - val_root_mean_squared_error: 1.1821\n",
      "Epoch 27/1000\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 0.0031 - root_mean_squared_error: 0.4861 - val_loss: 1.3449 - val_root_mean_squared_error: 1.1597\n",
      "Epoch 28/1000\n",
      "56/56 [==============================] - 12s 213ms/step - loss: 0.0034 - root_mean_squared_error: 0.4995 - val_loss: 1.4330 - val_root_mean_squared_error: 1.1971\n",
      "Epoch 29/1000\n",
      "56/56 [==============================] - 12s 213ms/step - loss: 0.0032 - root_mean_squared_error: 0.4864 - val_loss: 1.4146 - val_root_mean_squared_error: 1.1894\n",
      "Epoch 30/1000\n",
      "56/56 [==============================] - 12s 219ms/step - loss: 0.0030 - root_mean_squared_error: 0.4767 - val_loss: 1.3666 - val_root_mean_squared_error: 1.1690\n",
      "Epoch 31/1000\n",
      "56/56 [==============================] - 13s 226ms/step - loss: 0.0032 - root_mean_squared_error: 0.4824 - val_loss: 1.3314 - val_root_mean_squared_error: 1.1539\n",
      "Epoch 32/1000\n",
      "56/56 [==============================] - 12s 207ms/step - loss: 0.0029 - root_mean_squared_error: 0.4640 - val_loss: 1.3486 - val_root_mean_squared_error: 1.1613\n",
      "Epoch 33/1000\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.0030 - root_mean_squared_error: 0.4657 - val_loss: 1.3792 - val_root_mean_squared_error: 1.1744\n",
      "Epoch 34/1000\n",
      "56/56 [==============================] - 12s 217ms/step - loss: 0.0031 - root_mean_squared_error: 0.4747 - val_loss: 1.4305 - val_root_mean_squared_error: 1.1960\n",
      "Epoch 35/1000\n",
      "56/56 [==============================] - 12s 215ms/step - loss: 0.0029 - root_mean_squared_error: 0.4706 - val_loss: 1.3699 - val_root_mean_squared_error: 1.1704\n",
      "Epoch 36/1000\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.0027 - root_mean_squared_error: 0.4494 - val_loss: 1.3495 - val_root_mean_squared_error: 1.1617\n",
      "Epoch 37/1000\n",
      "56/56 [==============================] - 13s 224ms/step - loss: 0.0029 - root_mean_squared_error: 0.4614 - val_loss: 1.3211 - val_root_mean_squared_error: 1.1494\n",
      "Epoch 38/1000\n",
      "56/56 [==============================] - 13s 222ms/step - loss: 0.0027 - root_mean_squared_error: 0.4424 - val_loss: 1.3117 - val_root_mean_squared_error: 1.1453\n",
      "Epoch 39/1000\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.0023 - root_mean_squared_error: 0.4207 - val_loss: 1.2855 - val_root_mean_squared_error: 1.1338\n",
      "Epoch 40/1000\n",
      "56/56 [==============================] - 12s 217ms/step - loss: 0.0025 - root_mean_squared_error: 0.4238 - val_loss: 1.3212 - val_root_mean_squared_error: 1.1495\n",
      "Epoch 41/1000\n",
      "56/56 [==============================] - 12s 209ms/step - loss: 0.0025 - root_mean_squared_error: 0.4341 - val_loss: 1.3858 - val_root_mean_squared_error: 1.1772\n",
      "Epoch 42/1000\n",
      "56/56 [==============================] - 12s 214ms/step - loss: 0.0027 - root_mean_squared_error: 0.4402 - val_loss: 1.3634 - val_root_mean_squared_error: 1.1676\n",
      "Epoch 43/1000\n",
      "56/56 [==============================] - 12s 215ms/step - loss: 0.0026 - root_mean_squared_error: 0.4434 - val_loss: 1.3656 - val_root_mean_squared_error: 1.1686\n",
      "Epoch 44/1000\n",
      "56/56 [==============================] - 12s 216ms/step - loss: 0.0027 - root_mean_squared_error: 0.4382 - val_loss: 1.2951 - val_root_mean_squared_error: 1.1380\n",
      "Epoch 45/1000\n",
      "56/56 [==============================] - 12s 212ms/step - loss: 0.0023 - root_mean_squared_error: 0.4186 - val_loss: 1.3531 - val_root_mean_squared_error: 1.1632\n",
      "Epoch 46/1000\n",
      "56/56 [==============================] - 12s 208ms/step - loss: 0.0026 - root_mean_squared_error: 0.4330 - val_loss: 1.3167 - val_root_mean_squared_error: 1.1475\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 12s 206ms/step - loss: 0.0027 - root_mean_squared_error: 0.4370 - val_loss: 1.3411 - val_root_mean_squared_error: 1.1581\n",
      "Epoch 48/1000\n",
      "56/56 [==============================] - 12s 210ms/step - loss: 0.0025 - root_mean_squared_error: 0.4248 - val_loss: 1.3633 - val_root_mean_squared_error: 1.1676\n",
      "Epoch 49/1000\n",
      "56/56 [==============================] - 13s 233ms/step - loss: 0.0025 - root_mean_squared_error: 0.4281 - val_loss: 1.3842 - val_root_mean_squared_error: 1.1765\n",
      "Epoch 50/1000\n",
      "56/56 [==============================] - 13s 237ms/step - loss: 0.0024 - root_mean_squared_error: 0.4224 - val_loss: 1.3579 - val_root_mean_squared_error: 1.1653\n",
      "Epoch 51/1000\n",
      "56/56 [==============================] - 13s 234ms/step - loss: 0.0022 - root_mean_squared_error: 0.4056 - val_loss: 1.3587 - val_root_mean_squared_error: 1.1656\n",
      "Epoch 52/1000\n",
      "56/56 [==============================] - 13s 238ms/step - loss: 0.0024 - root_mean_squared_error: 0.4084 - val_loss: 1.3280 - val_root_mean_squared_error: 1.1524\n",
      "Epoch 53/1000\n",
      "56/56 [==============================] - 14s 240ms/step - loss: 0.0026 - root_mean_squared_error: 0.4258 - val_loss: 1.4448 - val_root_mean_squared_error: 1.2020\n",
      "Epoch 54/1000\n",
      "56/56 [==============================] - 14s 246ms/step - loss: 0.0025 - root_mean_squared_error: 0.4221 - val_loss: 1.4201 - val_root_mean_squared_error: 1.1917\n",
      "Epoch 55/1000\n",
      "56/56 [==============================] - 13s 234ms/step - loss: 0.0025 - root_mean_squared_error: 0.4211 - val_loss: 1.3328 - val_root_mean_squared_error: 1.1545\n",
      "Epoch 56/1000\n",
      "56/56 [==============================] - 14s 255ms/step - loss: 0.0023 - root_mean_squared_error: 0.4006 - val_loss: 1.3241 - val_root_mean_squared_error: 1.1507\n",
      "Epoch 57/1000\n",
      "56/56 [==============================] - 14s 255ms/step - loss: 0.0023 - root_mean_squared_error: 0.3970 - val_loss: 1.3578 - val_root_mean_squared_error: 1.1653\n",
      "Epoch 58/1000\n",
      "56/56 [==============================] - 14s 254ms/step - loss: 0.0021 - root_mean_squared_error: 0.3898 - val_loss: 1.3076 - val_root_mean_squared_error: 1.1435\n",
      "Epoch 59/1000\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.0022 - root_mean_squared_error: 0.3954Restoring model weights from the end of the best epoch: 39.\n",
      "56/56 [==============================] - 13s 239ms/step - loss: 0.0022 - root_mean_squared_error: 0.3954 - val_loss: 1.3107 - val_root_mean_squared_error: 1.1449\n",
      "Epoch 00059: early stopping\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(\n",
    "    x = [tf.constant(train_text)],\n",
    "    y = {'out1y':np.array(train_cost)},\n",
    "    sample_weight={'out1y': np.array(weights)},\n",
    "    epochs=epochs,\n",
    "    batch_size = batch_size,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(\n",
    "        [tf.constant(val_text)],\n",
    "        {'out1y':np.array(val_cost)}),\n",
    "    callbacks=[early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pl4_VMfLyVPe",
    "outputId": "2c27578b-333f-4fdf-ac14-65d9ab59438f"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "n = 2\n",
    "# a = 0.5\n",
    "output = pd.DataFrame()\n",
    "\n",
    "test_results0 = model.predict(tf.constant(test_text))\n",
    "test_result= test_results0.reshape(len(test_text)).tolist()\n",
    "test_cost = test_df[var]\n",
    "\n",
    "difference_array = np.subtract(test_result, test_cost)\n",
    "squared_array = np.square(difference_array)\n",
    "mse = squared_array.mean()\n",
    "\n",
    "correlation_matrix = np.corrcoef(test_cost, test_result)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2\n",
    "\n",
    "output.loc[idx,'n'] = n\n",
    "output.loc[idx,'var'] = var\n",
    "output.loc[idx,'lr'] = init_lr\n",
    "output.loc[idx,'mse'] = mse\n",
    "output.loc[idx,'r^2'] = r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "KSIBemXcyVPe",
    "outputId": "142b1a76-fb0b-485e-d466-c36035cd3790",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>var</th>\n",
       "      <th>lr</th>\n",
       "      <th>mse</th>\n",
       "      <th>r^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Unique_expert</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>1.289976</td>\n",
       "      <td>0.423234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n            var       lr       mse       r^2\n",
       "0  2.0  Unique_expert  0.00002  1.289976  0.423234"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "FYTeMDeYyVPf"
   },
   "outputs": [],
   "source": [
    "##save and load models\n",
    "##https://stackoverflow.com/questions/46610732/how-to-freeze-some-layers-when-fine-tune-resnet50\n",
    "\n",
    "initial_lrate = 0.000005\n",
    "\n",
    "# var = 'Unique_expert'\n",
    "# var = 'Drawing_expert'\n",
    "# var = 'Useful_expert'\n",
    "# var = 'Elegence_expert'\n",
    "# var = 'Creativity_expert'\n",
    "\n",
    "# model_name = 'model weight/'+var+'/new sketch-'+var+'-best.h5'\n",
    "# model_name = 'model weight/'+var+'/both_ratio-'+str(a)+'-'+var+'-best.h5'\n",
    "# model_name = 'model weight/'+var+'/simple_joint_ratio-'+str(a)+'-'+var+'-best.h5'\n",
    "# model_name = 'model weight/'+var+'/joint_lr-'+str(initial_lrate)+'-'+var+'-best.h5'\n",
    "model_name = 'model weight/'+var+'/new text-'+var+'-best.h5'\n",
    "# model.save(model_name, save_format='h5')\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model(model_name, custom_objects={'KerasLayer':hub.KerasLayer,'AdamWeightDecay': optimizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch1 = sketch1.astype({\"text\": str})\n",
    "text = list(sketch1['text'])\n",
    "\n",
    "from keras import backend as K\n",
    "sec_last = model.layers[2]\n",
    "get_output = K.function([model.layers[0].input, ],[sec_last.output, model.layers[-1].output])\n",
    "[emb_text, predictions] = get_output(tf.constant(text))\n",
    "\n",
    "np.savetxt('text emb 1-'+var, emb_text['pooled_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cj7iRDbCYiZu"
   },
   "outputs": [],
   "source": [
    "# ##to test which BEET\n",
    "# idx = 0\n",
    "# output = pd.DataFrame()\n",
    "# for key in map_name_to_handle.keys():\n",
    "#     bert_model_name = key\n",
    "#     tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "#     tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]   \n",
    "#     text_input = Input(shape=(), dtype=tf.string, name='text')\n",
    "#     preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "#     encoder_inputs = preprocessing_layer(text_input)\n",
    "#     encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "#     outputs = encoder(encoder_inputs)\n",
    "#     net = outputs['pooled_output']\n",
    "#     # net = layers.Dropout(0.1)(net)\n",
    "#     net = layers.Dense(128, activation='relu')(net)\n",
    "#     net = layers.Dropout(0.1)(net)\n",
    "#     out1y = layers.Dense(1, activation='relu', name='out1y')(net)\n",
    "#     model = Model(text_input, out1y)\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss=tf.keras.losses.MeanSquaredError(),\n",
    "#                   metrics= tf.keras.metrics.RootMeanSquaredError())\n",
    "#     training_history = model.fit(\n",
    "#         x = tf.constant(train_text),\n",
    "#         y = {'out1y':np.array(train_cost)},\n",
    "#         sample_weight={'out1y': np.array(weights)},\n",
    "#         epochs=epochs,\n",
    "#         batch_size = batch_size,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "#         validation_data=(\n",
    "#             tf.constant(val_text),\n",
    "#             {'out1y':np.array(val_cost)}),\n",
    "#         callbacks=[early_stopping_callback]\n",
    "#     )\n",
    "\n",
    "#     test_results0 = model.predict(tf.constant(test_text))\n",
    "#     test_result= test_results0.reshape(len(test_text)).tolist()\n",
    "#     test_cost = test_df[var]\n",
    "\n",
    "#     difference_array = np.subtract(test_result, test_cost)\n",
    "#     squared_array = np.square(difference_array)\n",
    "#     mse = squared_array.mean()\n",
    "\n",
    "#     correlation_matrix = np.corrcoef(test_cost, test_result)\n",
    "#     correlation_xy = correlation_matrix[0,1]\n",
    "#     r_squared = correlation_xy**2\n",
    "\n",
    "#     output.loc[idx,'n'] = idx\n",
    "#     output.loc[idx,'var'] = var\n",
    "#     output.loc[idx,'lr'] = init_lr\n",
    "#     output.loc[idx,'mse'] = mse\n",
    "#     output.loc[idx,'r^2'] = r_squared\n",
    "#     output.loc[idx,'model'] = key\n",
    "#     idx += 1\n",
    "#     print (key+\":  \", r_squared)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "attention - text.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "tf-transf",
   "language": "python",
   "name": "tf-transf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
